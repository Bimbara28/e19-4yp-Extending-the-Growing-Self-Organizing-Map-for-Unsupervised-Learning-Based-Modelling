{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "702a33b3",
   "metadata": {},
   "source": [
    "Prepare base dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aa2f86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, normalized_mutual_info_score\n",
    "from minisom import MiniSom  # pip install minisom if not yet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bf71c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline for gene\n",
    "# Load the normalized gene expression data (transposed: samples as rows)\n",
    "#df = pd.read_csv(\"example/data/GSE/GSE5281_normalized_gene_expression.csv\", index_col=0).T\n",
    "\n",
    "# Add fake metadata labels for comparison\n",
    "#df[\"Id\"] = df.index\n",
    "#df[\"Species\"] = [\"Alzheimer's Disease\" if i < 87 else \"Control\" for i in range(len(df))]\n",
    "\n",
    "#features_only = df.drop(columns=[\"Id\", \"Species\"])\n",
    "#true_labels = df[\"Species\"].replace({\"Alzheimer's Disease\": 1, \"Control\": 0}).to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0ec789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline for iris\n",
    "# Load the Iris dataset from CSV\n",
    "df = pd.read_csv(\"example/data/iris.csv\")\n",
    "\n",
    "# Fix column names if needed\n",
    "if \"species\" in df.columns:\n",
    "    df.rename(columns={\"species\": \"Species\"}, inplace=True)\n",
    "\n",
    "df[\"Id\"] = df.index.astype(str)\n",
    "\n",
    "# Extract features and true labels\n",
    "features_only = df.drop(columns=[\"Id\", \"Species\"])\n",
    "true_labels = df[\"Species\"].replace({\"setosa\": 0, \"versicolor\": 1, \"virginica\": 2}).to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e674c2d",
   "metadata": {},
   "source": [
    "k-Means + Hierarchical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "589dfa14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BIMBARA\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "c:\\Users\\BIMBARA\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "\n",
    "# Step 2a: k-means\n",
    "kmeans = KMeans(n_clusters=10, random_state=0)\n",
    "kmeans_labels = kmeans.fit_predict(features_only)\n",
    "\n",
    "# Step 2b: hierarchical on k-means cluster centers\n",
    "Z_kmeans = linkage(kmeans.cluster_centers_, method='average')\n",
    "final_clusters_kmeans = fcluster(Z_kmeans, 3, criterion='maxclust') \n",
    "\n",
    "# Assign final 2-cluster labels back to samples\n",
    "predicted_labels_kmeans = [final_clusters_kmeans[label] for label in kmeans_labels]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dee90ac",
   "metadata": {},
   "source": [
    "SOM + Hierarchical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56e7d9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3a: SOM\n",
    "som = MiniSom(x=10, y=10, input_len=features_only.shape[1], sigma=1.0, learning_rate=0.5, random_seed=42)\n",
    "som.train_random(features_only.to_numpy(), 100)\n",
    "\n",
    "# Get BMU coordinates for each sample\n",
    "som_coords = np.array([som.winner(x) for x in features_only.to_numpy()])\n",
    "som_coords_df = pd.DataFrame(som_coords, columns=[\"x\", \"y\"])\n",
    "\n",
    "# Step 3b: Hierarchical on SOM node coordinates\n",
    "Z_som = linkage(som_coords_df.to_numpy(), method='average')\n",
    "predicted_labels_som = fcluster(Z_som, 2, criterion='maxclust')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdb6339",
   "metadata": {},
   "source": [
    "DBSCAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc31cc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: DBSCAN\n",
    "dbscan = DBSCAN(eps=5, min_samples=5, metric='euclidean')\n",
    "predicted_labels_dbscan = dbscan.fit_predict(features_only)\n",
    "\n",
    "# Filter out noise (-1)\n",
    "valid_idx = predicted_labels_dbscan != -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b148c024",
   "metadata": {},
   "source": [
    "Evaluation Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a05c0c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_clustering(true, pred, X):\n",
    "    pred = np.array(pred)\n",
    "    mask = ~pd.isnull(pred)\n",
    "    if len(set(pred[mask])) > 1:\n",
    "        sil = silhouette_score(X[mask], pred[mask])\n",
    "        ari = adjusted_rand_score(true[mask], pred[mask])\n",
    "        nmi = normalized_mutual_info_score(true[mask], pred[mask])\n",
    "    else:\n",
    "        sil = ari = nmi = -1  # invalid\n",
    "    return sil, ari, nmi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9807f11f",
   "metadata": {},
   "source": [
    "Run Evaluation and Save Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76b0df1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "results = []\n",
    "\n",
    "# ✅ kMeans + Hierarchical\n",
    "sil, ari, nmi = evaluate_clustering(true_labels, predicted_labels_kmeans, features_only.to_numpy())\n",
    "ccc_kmeans, _ = cophenet(Z_kmeans, pdist(kmeans.cluster_centers_))\n",
    "results.append([\"kMeans + Hierarchical\", sil, ari, nmi, ccc_kmeans])\n",
    "\n",
    "# ✅ SOM + Hierarchical\n",
    "sil, ari, nmi = evaluate_clustering(true_labels, predicted_labels_som, features_only.to_numpy())\n",
    "ccc_som, _ = cophenet(Z_som, pdist(som_coords_df.to_numpy()))\n",
    "results.append([\"SOM + Hierarchical\", sil, ari, nmi, ccc_som])\n",
    "\n",
    "# ✅ DBSCAN (no CCC)\n",
    "sil, ari, nmi = evaluate_clustering(\n",
    "    true_labels[valid_idx],\n",
    "    predicted_labels_dbscan[valid_idx],\n",
    "    features_only.to_numpy()[valid_idx]\n",
    ")\n",
    "results.append([\"DBSCAN\", sil, ari, nmi, None])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d30f3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Comparison results with CCC saved to 'results/clustering_comparison_metrics_iris.csv'\n"
     ]
    }
   ],
   "source": [
    "# Save results with CCC\n",
    "results_df = pd.DataFrame(results, columns=[\"Method\", \"Silhouette\", \"ARI\", \"NMI\", \"CCC\"])\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "results_df.to_csv(\"results/clustering_comparison_metrics_iris.csv\", index=False)\n",
    "\n",
    "print(\"✅ Comparison results with CCC saved to 'results/clustering_comparison_metrics_iris.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
